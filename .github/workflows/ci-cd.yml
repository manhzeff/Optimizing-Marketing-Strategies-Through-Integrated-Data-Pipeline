# .github/workflows/ci-cd.yml
name: CI-CD Pipeline

on:
  push:
    branches: [ "dev-manh" ]
  pull_request:
    branches: [ "main" ]

jobs:
  end_to_end_pipeline:
    runs-on: ubuntu-latest

    steps:
      # 1. Check out mã nguồn
      - name: Check out repo
        uses: actions/checkout@v3

      # 2. Thiết lập Docker Buildx (nếu cần build image)
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      # 3. Create .env file
      - name: Create .env file from Secrets
        run: |
          echo "AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}" >> .env
          echo "AWS_SECRET_KEY_ID=${{ secrets.AWS_SECRET_KEY_ID }}" >> .env
          echo "AWS_S3_BUCKET=${{ secrets.AWS_S3_BUCKET }}" >> .env
          echo "SNOWFLAKE_ACCOUNT=${{ secrets.SNOWFLAKE_ACCOUNT }}" >> .env
          echo "SNOWFLAKE_USER=${{ secrets.SNOWFLAKE_USER }}" >> .env
          echo "SNOWFLAKE_PASSWORD=${{ secrets.SNOWFLAKE_PASSWORD }}" >> .env
          echo "SNOWFLAKE_DATABASE=${{ secrets.SNOWFLAKE_DATABASE }}" >> .env
          echo "SNOWFLAKE_SCHEMA=${{ secrets.SNOWFLAKE_SCHEMA }}" >> .env
          echo "SNOWFLAKE_TABLE=${{ secrets.SNOWFLAKE_TABLE }}" >> .env
          echo "SNOWFLAKE_WAREHOUSE=${{ secrets.SNOWFLAKE_WAREHOUSE }}" >> .env

      - name: Set up Python environment and install dependencies
        run: |
          python3 -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install -r requirements.txt


      - name: Start Airflow via Docker Compose
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose --env-file .env up -d

      - name: Verify Airflow DAGs
        run: |
          echo "Checking if DAG 'data_ingestion_aws_project' exists..."
          dag_list=$(docker exec -i dtc-de-airflow-webserver-1 airflow dags list | grep 'data_ingestion_aws_project' || true)
          if [ -z "$dag_list" ]; then
            echo "Error: DAG 'data_ingestion_aws_project' not found."
            exit 1
          else
            echo "DAG 'data_ingestion_aws_project' is available."
          fi
      

      - name: Trigger Airflow DAG
        run: |
          echo "Triggering Airflow DAG..."
          docker exec -i dtc-de-airflow-webserver-1 airflow dags trigger data_ingestion_aws_project



      - name: Monitor Airflow DAG
        run: |
          echo "Monitoring DAG execution progress..."
          while true; do
            echo "Fetching DAG runs for 'data_ingestion_aws_project'..."
            docker exec -i dtc-de-airflow-webserver-1 airflow dags list-runs --dag-id data_ingestion_aws_project
            state=$(docker exec -i dtc-de-airflow-webserver-1 airflow dags list-runs --dag-id data_ingestion_aws_project --state running | grep running || true)
            if [ -z "$state" ]; then
              state_queued=$(docker exec -i dtc-de-airflow-webserver-1 airflow dags list-runs --dag-id data_ingestion_aws_project --state queued | grep queued || true)
              if [ -z "$state_queued" ]; then
                echo "DAG execution completed or no queued tasks remain."
                docker exec -i dtc-de-airflow-webserver-1 airflow dags list-runs --dag-id data_ingestion_aws_project
                break
              else
                echo "DAG is still queued. Investigating potential issues..."
                echo "Checking Scheduler logs..."
                docker logs dtc-de-airflow-scheduler-1 | tail -n 20
                echo "Checking Webserver logs..."
                docker logs dtc-de-airflow-webserver-1 | tail -n 20
                sleep 10
              fi
            else
              echo "DAG is running. Checking again in 10 seconds..."
              sleep 10
            fi
          done
          

      - name: Teardown Airflow
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose down

      # -----------------------------------------------------------
      # 4. Khởi động Kafka & Chạy producer + consumer
      # -----------------------------------------------------------
      - name: Start Kafka
        run: |
          cd sources/stream_processing
          docker compose --env-file .env up -d
          echo "Chờ Kafka khởi động..."
          sleep 15

      - name: Run Kafka Producer
        run: |
          cd sources/stream_processing
          python producer.py

      - name: Run Kafka Consumer
        run: |
          cd sources/stream_processing
          python consumer.py

      - name: Teardown Kafka
        run: |
          cd sources/stream_processing
          docker compose down

      # -----------------------------------------------------------
      # 6. Cài dbt & chạy dbt run (DWH & Data Mart)
      # -----------------------------------------------------------
      - name: Install dbt
        run: |
          pip install dbt-core dbt-snowflake
          # Hoặc dbt-bigquery, dbt-postgres... tùy DWH

      - name: dbt run for marketing_campaign_dwh
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_KEY_ID: ${{ secrets.AWS_SECRET_KEY_ID }}
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
          SNOWFLAKE_TABLE: ${{ secrets.SNOWFLAKE_TABLE }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
        run: |
            cd sources/batch_processing/dbt/marketing_campaign_dwh
            dbt deps
            dbt run
            dbt test
        
      - name: dbt run for marketing_datamart
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_KEY_ID: ${{ secrets.AWS_SECRET_KEY_ID }}
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
          SNOWFLAKE_TABLE: ${{ secrets.SNOWFLAKE_TABLE }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
        run: |
            cd sources/batch_processing/dbt/marketing_datamart
            dbt deps
            dbt run
            dbt test
