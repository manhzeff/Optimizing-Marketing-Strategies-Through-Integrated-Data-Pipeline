# .github/workflows/ci-cd.yml
name: CI-CD Pipeline

on:
  push:
    branches: [ "dev-manh" ]
  pull_request:
    branches: [ "main" ]

jobs:
  end_to_end_pipeline:
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_KEY_ID: ${{ secrets.AWS_SECRET_KEY_ID }}
      AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
      SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
      SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
      SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
      SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
      SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
      SNOWFLAKE_TABLE: ${{ secrets.SNOWFLAKE_TABLE }}
      SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}

    steps:
      - name: Check out repo
        uses: actions/checkout@v3

      - name: Cache pip
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt



      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Create .env file from Secrets
        run: |
          cat <<EOF > .env
          AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
          AWS_SECRET_KEY_ID=${AWS_SECRET_KEY_ID}
          AWS_S3_BUCKET=${AWS_S3_BUCKET}
          SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
          SNOWFLAKE_USER=${SNOWFLAKE_USER}
          SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
          SNOWFLAKE_DATABASE=${SNOWFLAKE_DATABASE}
          SNOWFLAKE_SCHEMA=${SNOWFLAKE_SCHEMA}
          SNOWFLAKE_TABLE=${SNOWFLAKE_TABLE}
          SNOWFLAKE_WAREHOUSE=${SNOWFLAKE_WAREHOUSE}
          EOF

      - name: Start Airflow
        run: |
            cd sources/batch_processing/airflow/dag-airflow
            docker compose --env-file .env up -d
            echo "ch·ªù airflow to start"
            sleep 120

      - name: Verify Airflow Services
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose ps

        

      - name: Trigger DAG via Airflow API
        id: trigger_dag
        run: |
            docker exec -i dtc-de-airflow-webserver-1 airflow trigger_dag data_ingestion_aws_project





      - name: Teardown Airflow
        if: always()
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose down

      - name: Start Kafka
        run: |
          cd sources/stream_processing
          docker compose --env-file .env up -d
          echo "Waiting for Kafka to start..."
          sleep 15

      - name: Run Kafka Producer
        run: |
          cd sources/stream_processing
          python producer.py

      - name: Run Kafka Consumer
        run: |
          cd sources/stream_processing
          python consumer.py

      - name: Teardown Kafka
        if: always()
        run: |
          cd sources/stream_processing
          docker compose down

      - name: Cache dbt packages
        uses: actions/cache@v3
        with:
          path: ~/.dbt
          key: ${{ runner.os }}-dbt-${{ hashFiles('**/packages.yml') }}
          restore-keys: |
            ${{ runner.os }}-dbt-

      - name: Install dbt
        run: |
          pip install dbt-core dbt-snowflake

      - name: dbt run for DWH
        run: |
          cd sources/batch_processing/dbt/marketing_campaign_dwh
          dbt deps
          dbt run
          dbt test

      - name: dbt run for Data Mart
        run: |
          cd sources/batch_processing/dbt/marketing_datamart
          dbt deps
          dbt run
          dbt test

