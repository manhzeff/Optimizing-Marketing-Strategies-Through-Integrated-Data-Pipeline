# .github/workflows/ci-cd.yml
name: CI-CD Pipeline

on:
  push:
    branches: [ "dev-manh" ]
  pull_request:
    branches: [ "main" ]

jobs:
  end_to_end_pipeline:
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_KEY_ID: ${{ secrets.AWS_SECRET_KEY_ID }}
      AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
      SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
      SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
      SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
      SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
      SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
      SNOWFLAKE_TABLE: ${{ secrets.SNOWFLAKE_TABLE }}
      SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}

    steps:
      - name: Check out repo
        uses: actions/checkout@v3

      - name: Cache pip
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install jq
        run: |
            sudo apt-get update
            sudo apt-get install -y jq
        

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt



      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Create .env file from Secrets
        run: |
          cat <<EOF > .env
          AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
          AWS_SECRET_KEY_ID=${AWS_SECRET_KEY_ID}
          AWS_S3_BUCKET=${AWS_S3_BUCKET}
          SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
          SNOWFLAKE_USER=${SNOWFLAKE_USER}
          SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
          SNOWFLAKE_DATABASE=${SNOWFLAKE_DATABASE}
          SNOWFLAKE_SCHEMA=${SNOWFLAKE_SCHEMA}
          SNOWFLAKE_TABLE=${SNOWFLAKE_TABLE}
          SNOWFLAKE_WAREHOUSE=${SNOWFLAKE_WAREHOUSE}
          EOF

      - name: Start Airflow
        run: |
            cd sources/batch_processing/airflow/dag-airflow
            docker compose --env-file .env up -d
            echo "ch·ªù airflow to start"
            sleep 60

      - name: Verify Airflow Services
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose ps

        

      - name: Trigger DAG via Airflow API
        id: trigger_dag
        run: |
            AIRFLOW_HOST=localhost
            AIRFLOW_PORT=8080
            DAG_ID=data_ingestion_aws_project
            AUTH_HEADER="Authorization: Basic $(echo -n 'airflow:airflow' | base64)"
  
            # Trigger the DAG and extract the dag_run_id
            DAG_RUN_RESPONSE=$(curl -s -X POST "http://$AIRFLOW_HOST:$AIRFLOW_PORT/api/v1/dags/$DAG_ID/dagRuns" \
              -H "Content-Type: application/json" \
              -H "$AUTH_HEADER" \
              -d '{}')
            
            echo "DAG Run Response: $DAG_RUN_RESPONSE"
            DAG_RUN_ID=$(echo "$DAG_RUN_RESPONSE" | jq -r '.dag_run_id')
            echo "DAG Run ID: $DAG_RUN_ID"
  
            # Wait for the DAG to run
            sleep 120
  
      - name: Check DAG Run Status
        run: |
            AIRFLOW_HOST=localhost
            AIRFLOW_PORT=8080
            DAG_ID=data_ingestion_aws_project
            AUTH_HEADER="Authorization: Basic $(echo -n 'airflow:airflow' | base64)"
            DAG_RUN_ID=${DAG_RUN_ID}
  
            # Check the status of the DAG Run
            DAG_STATUS=$(curl -s -X GET "http://$AIRFLOW_HOST:$AIRFLOW_PORT/api/v1/dags/$DAG_ID/dagRuns/$DAG_RUN_ID" \
              -H "$AUTH_HEADER" | jq -r '.state')
            echo "DAG Run Status: $DAG_STATUS"
  
            if [ "$DAG_STATUS" != "success" ]; then
              echo "DAG Run failed or is not successful!"
              exit 1
            fi
  
        


      - name: Teardown Airflow
        if: always()
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose down

      - name: Start Kafka
        run: |
          cd sources/stream_processing
          docker compose --env-file .env up -d
          echo "Waiting for Kafka to start..."
          sleep 15

      - name: Run Kafka Producer
        run: |
          cd sources/stream_processing
          python producer.py

      - name: Run Kafka Consumer
        run: |
          cd sources/stream_processing
          python consumer.py

      - name: Teardown Kafka
        if: always()
        run: |
          cd sources/stream_processing
          docker compose down

      - name: Cache dbt packages
        uses: actions/cache@v3
        with:
          path: ~/.dbt
          key: ${{ runner.os }}-dbt-${{ hashFiles('**/packages.yml') }}
          restore-keys: |
            ${{ runner.os }}-dbt-

      - name: Install dbt
        run: |
          pip install dbt-core dbt-snowflake

      - name: dbt run for DWH
        run: |
          cd sources/batch_processing/dbt/marketing_campaign_dwh
          dbt deps
          dbt run
          dbt test

      - name: dbt run for Data Mart
        run: |
          cd sources/batch_processing/dbt/marketing_datamart
          dbt deps
          dbt run
          dbt test

