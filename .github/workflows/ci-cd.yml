# .github/workflows/ci-cd.yml
name: CI-CD Pipeline

on:
  push:
    branches: [ "dev-manh" ]
  pull_request:
    branches: [ "main" ]

jobs:
  end_to_end_pipeline:
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_KEY_ID: ${{ secrets.AWS_SECRET_KEY_ID }}
      AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
      SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
      SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
      SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
      SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
      SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
      SNOWFLAKE_TABLE: ${{ secrets.SNOWFLAKE_TABLE }}
      SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}

    steps:
      - name: Check out repo
        uses: actions/checkout@v3

      - name: Cache pip
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Create .env file from Secrets
        run: |
          cat <<EOF > .env
          AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
          AWS_SECRET_KEY_ID=${AWS_SECRET_KEY_ID}
          AWS_S3_BUCKET=${AWS_S3_BUCKET}
          SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
          SNOWFLAKE_USER=${SNOWFLAKE_USER}
          SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
          SNOWFLAKE_DATABASE=${SNOWFLAKE_DATABASE}
          SNOWFLAKE_SCHEMA=${SNOWFLAKE_SCHEMA}
          SNOWFLAKE_TABLE=${SNOWFLAKE_TABLE}
          SNOWFLAKE_WAREHOUSE=${SNOWFLAKE_WAREHOUSE}
          EOF

      - name: Start Airflow
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose --env-file .env up -d

      - name: Wait for Airflow
        run: |
            for i in {1..60}; do
              if curl -sSf http://localhost:8080/health > /dev/null; then
                echo "Airflow is up!"
                break
              fi
              echo "Waiting for Airflow... ($i/60)"
              sleep 5
            done
            if ! curl -sSf http://localhost:8080/health > /dev/null; then
              echo "Airflow failed to start."
              exit 1
            fi


      - name: Verify Airflow Services
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose ps

      - name: Fetch Scheduler Logs
        if: failure()
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose logs scheduler > scheduler.log
          cat scheduler.log

      - name: Verify and Trigger DAG
        run: |
          webserver_container=$(docker ps --filter "name=webserver" --format "{{.ID}}")
          if docker exec $webserver_container airflow dags list | grep -q 'data_ingestion_aws_project'; then
            echo "DAG found. Triggering..."
            docker exec -i $webserver_container airflow dags trigger data_ingestion_aws_project
          else
            echo "DAG not found."
            exit 1
          fi

      - name: Monitor DAG
        run: |
          timeout=600
          interval=10
          elapsed=0
          webserver_container=$(docker ps --filter "name=webserver" --format "{{.ID}}")
          while [ $elapsed -lt $timeout ]; do
            dag_state=$(docker exec $webserver_container airflow dags state data_ingestion_aws_project $(date -Iseconds))
            echo "Current DAG state: $dag_state"
            if [[ "$dag_state" == "success" ]]; then
              echo "DAG execution completed successfully."
              break
            elif [[ "$dag_state" == "failed" ]]; then
              echo "DAG execution failed."
              exit 1
            fi
            echo "DAG is still running. Checking again in $interval seconds..."
            sleep $interval
            elapsed=$((elapsed + interval))
          done
          if [ $elapsed -ge $timeout ]; then
            echo "DAG execution timed out."
            exit 1
          fi

      - name: Teardown Airflow
        if: always()
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose down

      - name: Start Kafka
        run: |
          cd sources/stream_processing
          docker compose --env-file .env up -d
          echo "Waiting for Kafka to start..."
          sleep 15

      - name: Run Kafka Producer
        run: |
          cd sources/stream_processing
          python producer.py

      - name: Run Kafka Consumer
        run: |
          cd sources/stream_processing
          python consumer.py

      - name: Teardown Kafka
        if: always()
        run: |
          cd sources/stream_processing
          docker compose down

      - name: Cache dbt packages
        uses: actions/cache@v3
        with:
          path: ~/.dbt
          key: ${{ runner.os }}-dbt-${{ hashFiles('**/packages.yml') }}
          restore-keys: |
            ${{ runner.os }}-dbt-

      - name: Install dbt
        run: |
          pip install dbt-core dbt-snowflake

      - name: dbt run for DWH
        run: |
          cd sources/batch_processing/dbt/marketing_campaign_dwh
          dbt deps
          dbt run
          dbt test

      - name: dbt run for Data Mart
        run: |
          cd sources/batch_processing/dbt/marketing_datamart
          dbt deps
          dbt run
          dbt test

