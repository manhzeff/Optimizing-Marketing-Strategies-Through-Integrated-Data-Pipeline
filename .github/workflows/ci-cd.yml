# .github/workflows/ci-cd.yml
name: CI-CD Pipeline

on:
  push:
    branches: [ "dev-manh" ]
  pull_request:
    branches: [ "main" ]

jobs:
  end_to_end_pipeline:
    runs-on: ubuntu-latest

    steps:
      # 1. Check out mã nguồn
      - name: Check out repo
        uses: actions/checkout@v3

      # 2. Thiết lập Docker Buildx (nếu cần build image)
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      # 3. Create .env file
      - name: Create .env file from Secrets
        run: |
          echo "AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}" >> .env
          echo "AWS_SECRET_KEY_ID=${{ secrets.AWS_SECRET_KEY_ID }}" >> .env
          echo "AWS_S3_BUCKET=${{ secrets.AWS_S3_BUCKET }}" >> .env
          echo "SNOWFLAKE_ACCOUNT=${{ secrets.SNOWFLAKE_ACCOUNT }}" >> .env
          echo "SNOWFLAKE_USER=${{ secrets.SNOWFLAKE_USER }}" >> .env
          echo "SNOWFLAKE_PASSWORD=${{ secrets.SNOWFLAKE_PASSWORD }}" >> .env
          echo "SNOWFLAKE_DATABASE=${{ secrets.SNOWFLAKE_DATABASE }}" >> .env
          echo "SNOWFLAKE_SCHEMA=${{ secrets.SNOWFLAKE_SCHEMA }}" >> .env
          echo "SNOWFLAKE_TABLE=${{ secrets.SNOWFLAKE_TABLE }}" >> .env
          echo "SNOWFLAKE_WAREHOUSE=${{ secrets.SNOWFLAKE_WAREHOUSE }}" >> .env

      - name: Set up Python environment and install dependencies
        run: |
          python3 -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install -r requirements.txt


      - name: Start Airflow via Docker Compose
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose --env-file .env up -d


      - name: Verify Airflow DAGs
        run: |
          echo "Checking if DAG 'data_ingestion_aws_project' exists..."
          dag_list=$(docker exec -i dtc-de-airflow-webserver-1 airflow dags list | grep 'data_ingestion_aws_project' || true)
          if [ -z "$dag_list" ]; then
            echo "Error: DAG 'data_ingestion_aws_project' not found."
            exit 1
          else
            echo "DAG 'data_ingestion_aws_project' is available."
          fi
      
      - name: Check and Restart Scheduler and Worker
        run: |
            echo "Checking Airflow Scheduler and Worker status..."
  
            # Check Scheduler
            echo "Checking Scheduler..."
            scheduler_status=$(docker exec -i dtc-de-airflow-webserver-1 ps aux | grep -v grep | grep airflow-scheduler || true)
            if [ -z "$scheduler_status" ]; then
              echo "Scheduler is not running. Restarting Scheduler..."
              docker exec -i dtc-de-airflow-webserver-1 pkill -f 'airflow scheduler' || true
              docker exec -i dtc-de-airflow-webserver-1 airflow scheduler &
              sleep 5
            else
              echo "Scheduler is running."
            fi
  
            # Check Worker (for CeleryExecutor)
            echo "Checking Worker..."
            worker_status=$(docker exec -i dtc-de-airflow-worker-1 ps aux | grep -v grep | grep 'celery worker' || true)
            if [ -z "$worker_status" ]; then
              echo "Worker is not running. Cleaning up stale processes and restarting Worker..."
              docker exec -i dtc-de-airflow-worker-1 pkill -f 'celery worker' || true
              docker exec -i dtc-de-airflow-worker-1 rm -f /opt/airflow/airflow-worker.pid || true
              docker exec -i dtc-de-airflow-worker-1 airflow celery worker &
              sleep 5
            else
              echo "Worker is running."
            fi
  
            echo "Scheduler and Worker checks completed."
  




      - name: Trigger Airflow DAG
        run: |
          echo "Triggering Airflow DAG..."
          docker exec -i dtc-de-airflow-webserver-1 airflow dags trigger data_ingestion_aws_project



      - name: Monitor Airflow DAG
        run: |
          echo "Monitoring DAG execution progress..."
          while true; do
            echo "Fetching DAG runs for 'data_ingestion_aws_project'..."
            docker exec -i dtc-de-airflow-webserver-1 airflow dags list-runs --dag-id data_ingestion_aws_project
            state=$(docker exec -i dtc-de-airflow-webserver-1 airflow dags list-runs --state running --dag-id data_ingestion_aws_project | grep running || true)
            if [ -z "$state" ]; then
              echo "DAG execution completed."
              docker exec -i dtc-de-airflow-webserver-1 airflow dags list-runs --dag-id data_ingestion_aws_project
              break
            fi
            echo "DAG is still running. Checking again in 10 seconds..."
            sleep 10
          done
          

      - name: Teardown Airflow
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose down

      # -----------------------------------------------------------
      # 4. Khởi động Kafka & Chạy producer + consumer
      # -----------------------------------------------------------
      - name: Start Kafka
        run: |
          cd sources/stream_processing
          docker compose --env-file .env up -d
          echo "Chờ Kafka khởi động..."
          sleep 15

      - name: Run Kafka Producer
        run: |
          cd sources/stream_processing
          python producer.py

      - name: Run Kafka Consumer
        run: |
          cd sources/stream_processing
          python consumer.py

      - name: Teardown Kafka
        run: |
          cd sources/stream_processing
          docker compose down

      # -----------------------------------------------------------
      # 6. Cài dbt & chạy dbt run (DWH & Data Mart)
      # -----------------------------------------------------------
      - name: Install dbt
        run: |
          pip install dbt-core dbt-snowflake
          # Hoặc dbt-bigquery, dbt-postgres... tùy DWH

      - name: dbt run for marketing_campaign_dwh
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_KEY_ID: ${{ secrets.AWS_SECRET_KEY_ID }}
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
          SNOWFLAKE_TABLE: ${{ secrets.SNOWFLAKE_TABLE }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
        run: |
            cd sources/batch_processing/dbt/marketing_campaign_dwh
            dbt deps
            dbt run
            dbt test
        
      - name: dbt run for marketing_datamart
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_KEY_ID: ${{ secrets.AWS_SECRET_KEY_ID }}
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
          SNOWFLAKE_TABLE: ${{ secrets.SNOWFLAKE_TABLE }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
        run: |
            cd sources/batch_processing/dbt/marketing_datamart
            dbt deps
            dbt run
            dbt test
