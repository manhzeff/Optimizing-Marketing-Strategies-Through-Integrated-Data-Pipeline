# .github/workflows/ci-cd.yml
name: CI-CD Pipeline

on:
  push:
    branches: [ "dev-manh" ]
  pull_request:
    branches: [ "main" ]

jobs:
  end_to_end_pipeline:
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_KEY_ID: ${{ secrets.AWS_SECRET_KEY_ID }}
      AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
      SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
      SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
      SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
      SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
      SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
      SNOWFLAKE_TABLE: ${{ secrets.SNOWFLAKE_TABLE }}
      SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}

    steps:
      - name: Check out repo
        uses: actions/checkout@v3

      - name: Cache pip
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install jq
        run: |
            sudo apt-get update
            sudo apt-get install -y jq
        

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt



      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Create .env file from Secrets
        run: |
          cat <<EOF > .env
          AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
          AWS_SECRET_KEY_ID=${AWS_SECRET_KEY_ID}
          AWS_S3_BUCKET=${AWS_S3_BUCKET}
          SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
          SNOWFLAKE_USER=${SNOWFLAKE_USER}
          SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
          SNOWFLAKE_DATABASE=${SNOWFLAKE_DATABASE}
          SNOWFLAKE_SCHEMA=${SNOWFLAKE_SCHEMA}
          SNOWFLAKE_TABLE=${SNOWFLAKE_TABLE}
          SNOWFLAKE_WAREHOUSE=${SNOWFLAKE_WAREHOUSE}
          EOF

      - name: Start Airflow
        run: |
            cd sources/batch_processing/airflow/dag-airflow
            docker compose --env-file .env up -d
            echo "chờ airflow to start"
            sleep 120

      - name: Verify Airflow Services
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose ps

        

      - name: Trigger DAG via Airflow API
        id: trigger_dag
        run: |
            RESPONSE=$(docker exec -i dtc-de-airflow-webserver-1 airflow dags trigger "data_ingestion_aws_project" --conf '{"trigger_source": "github_actions"}')
            echo "Trigger response: $RESPONSE"
  
      - name: Extract DagRun ID
        id: extract_dagrun
        run: |
            # Giả sử RESPONSE là một chuỗi như: Created <DagRun data_ingestion_aws_project @ 2024-12-27T14:54:53+00:00: manual__2024-12-27T14:54:53+00:00, state:queued, ...>
            DAG_RUN_ID=$(echo "$RESPONSE" | grep -oP '(?<=@ ).*(?=: state)')
            echo "dag_run_id=$DAG_RUN_ID" >> $GITHUB_OUTPUT
  
      - name: Wait for DAG Run to Start Running
        run: |
            DAG_RUN_ID=${{ steps.extract_dagrun.outputs.dag_run_id }}
            MAX_RETRIES=30
            RETRY_COUNT=0
            SLEEP_TIME=10
  
            while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
              STATUS=$(curl -s -u $AIRFLOW_USERNAME:$AIRFLOW_PASSWORD "$AIRFLOW_API_URL/dags/data_ingestion_aws_project/dagRuns/$DAG_RUN_ID" | jq -r '.state')
              echo "Trạng thái hiện tại của DagRun: $STATUS"
              if [ "$STATUS" == "running" ] || [ "$STATUS" == "success" ]; then
                echo "DagRun đã bắt đầu chạy hoặc đã hoàn thành thành công."
                exit 0
              elif [ "$STATUS" == "failed" ] || [ "$STATUS" == "upstream_failed" ]; then
                echo "DagRun đã thất bại."
                exit 1
              fi
              echo "DagRun vẫn ở trạng thái '$STATUS'. Đang thử lại sau $SLEEP_TIME giây..."
              sleep $SLEEP_TIME
              RETRY_COUNT=$((RETRY_COUNT+1))
            done
  
            echo "DagRun không bắt đầu chạy trong thời gian mong đợi."
            exit 1




      - name: Teardown Airflow
        if: always()
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose down

      - name: Start Kafka
        run: |
          cd sources/stream_processing
          docker compose --env-file .env up -d
          echo "Waiting for Kafka to start..."
          sleep 15

      - name: Run Kafka Producer
        run: |
          cd sources/stream_processing
          python producer.py

      - name: Run Kafka Consumer
        run: |
          cd sources/stream_processing
          python consumer.py

      - name: Teardown Kafka
        if: always()
        run: |
          cd sources/stream_processing
          docker compose down

      - name: Cache dbt packages
        uses: actions/cache@v3
        with:
          path: ~/.dbt
          key: ${{ runner.os }}-dbt-${{ hashFiles('**/packages.yml') }}
          restore-keys: |
            ${{ runner.os }}-dbt-

      - name: Install dbt
        run: |
          pip install dbt-core dbt-snowflake

      - name: dbt run for DWH
        run: |
          cd sources/batch_processing/dbt/marketing_campaign_dwh
          dbt deps
          dbt run
          dbt test

      - name: dbt run for Data Mart
        run: |
          cd sources/batch_processing/dbt/marketing_datamart
          dbt deps
          dbt run
          dbt test

