# .github/workflows/ci-cd.yml
name: CI-CD Pipeline

on:
  push:
    branches: [ "dev-manh" ]
  pull_request:
    branches: [ "main" ]

jobs:
  end_to_end_pipeline:
    runs-on: ubuntu-latest

    steps:
      # 1. Check out mã nguồn
      - name: Check out repo
        uses: actions/checkout@v3

      # 2. Thiết lập Docker Buildx (nếu cần build image)
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      # 3. Create .env file
      - name: Create .env file from Secrets
        run: |
          echo "AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}" >> .env
          echo "AWS_SECRET_KEY_ID=${{ secrets.AWS_SECRET_KEY_ID }}" >> .env
          echo "AWS_S3_BUCKET=${{ secrets.AWS_S3_BUCKET }}" >> .env
          echo "SNOWFLAKE_ACCOUNT=${{ secrets.SNOWFLAKE_ACCOUNT }}" >> .env
          echo "SNOWFLAKE_USER=${{ secrets.SNOWFLAKE_USER }}" >> .env
          echo "SNOWFLAKE_PASSWORD=${{ secrets.SNOWFLAKE_PASSWORD }}" >> .env
          echo "SNOWFLAKE_DATABASE=${{ secrets.SNOWFLAKE_DATABASE }}" >> .env
          echo "SNOWFLAKE_SCHEMA=${{ secrets.SNOWFLAKE_SCHEMA }}" >> .env
          echo "SNOWFLAKE_TABLE=${{ secrets.SNOWFLAKE_TABLE }}" >> .env
          echo "SNOWFLAKE_WAREHOUSE=${{ secrets.SNOWFLAKE_WAREHOUSE }}" >> .env

      - name: Set up Python environment and install dependencies
        run: |
          python3 -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install -r requirements.txt


      - name: Start Airflow via Docker Compose
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose --env-file .env up -d


      - name: Wait for Airflow Scheduler to be Ready
        run: |
              echo "Đang kiểm tra Scheduler..."
              for i in {1..30}; do
                STATUS=$(docker exec dtc-de-airflow-scheduler-1 airflow scheduler status 2>&1)
                if echo "$STATUS" | grep -q "Scheduler is running"; then
                  echo "Scheduler đã sẵn sàng."
                  exit 0
                fi
                echo "Scheduler chưa sẵn sàng. Đang chờ..."
                sleep 5
              done
              echo "Scheduler không sẵn sàng sau 2.5 phút."
              exit 1

      - name: Verify Airflow DAGs
        run: |
          echo "Checking if DAG 'data_ingestion_aws_project' exists..."
          dag_list=$(docker exec -i dtc-de-airflow-webserver-1 airflow dags list | grep 'data_ingestion_aws_project' || true)
          if [ -z "$dag_list" ]; then
            echo "Error: DAG 'data_ingestion_aws_project' not found."
            exit 1
          else
            echo "DAG 'data_ingestion_aws_project' is available."
          fi
      

      - name: Trigger Airflow DAG
        run: |
          echo "Triggering Airflow DAG..."
          docker exec -i dtc-de-airflow-webserver-1 airflow dags trigger data_ingestion_aws_project



      - name: Monitor Airflow DAG
        run: |
          MAX_RETRIES=30
          COUNT=0
          while [ $COUNT -lt $MAX_RETRIES ]; do
            state=$(docker exec -i dtc-de-airflow-webserver-1 airflow dags list-runs --state running --dag-id data_ingestion_aws_project | grep running)
            if [ -z "$state" ]; then
              echo "DAG đã hoàn thành."
              docker exec -i dtc-de-airflow-webserver-1 airflow dags list-runs --dag-id data_ingestion_aws_project
              break
            fi
            echo "DAG vẫn đang chạy. Thử lại lần $((COUNT+1))/$MAX_RETRIES."
            COUNT=$((COUNT + 1))
            sleep 10
          done

          if [ $COUNT -eq $MAX_RETRIES ]; then
            echo "DAG không hoàn thành trong thời gian mong đợi."
            exit 1
          fi
          

      - name: Teardown Airflow
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose down

      # -----------------------------------------------------------
      # 4. Khởi động Kafka & Chạy producer + consumer
      # -----------------------------------------------------------
      - name: Start Kafka
        run: |
          cd sources/stream_processing
          docker compose --env-file .env up -d
          echo "Chờ Kafka khởi động..."
          sleep 15

      - name: Run Kafka Producer
        run: |
          cd sources/stream_processing
          python producer.py

      - name: Run Kafka Consumer
        run: |
          cd sources/stream_processing
          python consumer.py

      - name: Teardown Kafka
        run: |
          cd sources/stream_processing
          docker compose down

      # -----------------------------------------------------------
      # 6. Cài dbt & chạy dbt run (DWH & Data Mart)
      # -----------------------------------------------------------
      - name: Install dbt
        run: |
          pip install dbt-core dbt-snowflake
          # Hoặc dbt-bigquery, dbt-postgres... tùy DWH

      - name: dbt run for marketing_campaign_dwh
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_KEY_ID: ${{ secrets.AWS_SECRET_KEY_ID }}
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
          SNOWFLAKE_TABLE: ${{ secrets.SNOWFLAKE_TABLE }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
        run: |
            cd sources/batch_processing/dbt/marketing_campaign_dwh
            dbt deps
            dbt run
            dbt test
        
      - name: dbt run for marketing_datamart
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_KEY_ID: ${{ secrets.AWS_SECRET_KEY_ID }}
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
          SNOWFLAKE_TABLE: ${{ secrets.SNOWFLAKE_TABLE }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
        run: |
            cd sources/batch_processing/dbt/marketing_datamart
            dbt deps
            dbt run
            dbt test
