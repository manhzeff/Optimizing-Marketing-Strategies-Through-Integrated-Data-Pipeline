# .github/workflows/ci-cd.yml
name: CI-CD Pipeline

on:
  push:
    branches: [ "dev-manh" ]
  pull_request:
    branches: [ "main" ]

jobs:
  end_to_end_pipeline:
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_KEY_ID: ${{ secrets.AWS_SECRET_KEY_ID }}
      AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
      SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
      SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
      SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
      SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
      SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
      SNOWFLAKE_TABLE: ${{ secrets.SNOWFLAKE_TABLE }}
      SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}

    steps:
      - name: Check out repo
        uses: actions/checkout@v3

      - name: Cache pip
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt



      - name: Install jq
        run: |
          sudo apt-get update
          sudo apt-get install -y jq


      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Create .env file from Secrets
        run: |
          cat <<EOF > .env
          AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
          AWS_SECRET_KEY_ID=${AWS_SECRET_KEY_ID}
          AWS_S3_BUCKET=${AWS_S3_BUCKET}
          SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
          SNOWFLAKE_USER=${SNOWFLAKE_USER}
          SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
          SNOWFLAKE_DATABASE=${SNOWFLAKE_DATABASE}
          SNOWFLAKE_SCHEMA=${SNOWFLAKE_SCHEMA}
          SNOWFLAKE_TABLE=${SNOWFLAKE_TABLE}
          SNOWFLAKE_WAREHOUSE=${SNOWFLAKE_WAREHOUSE}
          EOF

      - name: Start Airflow
        run: |
            cd sources/batch_processing/airflow/dag-airflow
            docker compose --env-file .env up -d
            echo "Đang chờ Airflow khởi động..."
            for i in {1..240}; do  # Tối đa 20 phút (240 * 5 giây)
              # Kiểm tra trạng thái sức khỏe của tất cả các dịch vụ
              unhealthy_services=$(docker compose ps --filter "health!=healthy" --format "{{.Name}}: {{.Health}}")
              if [ -z "$unhealthy_services" ]; then
                echo "Tất cả các dịch vụ Airflow đã khỏe mạnh."
                break
              fi
              echo "Đang chờ các dịch vụ Airflow trở nên khỏe mạnh... ($i/240)"
              echo "Các dịch vụ chưa khỏe mạnh:"
              echo "$unhealthy_services"
              sleep 5
            done
            
            # Kiểm tra lần cuối để đảm bảo tất cả dịch vụ đã khỏe mạnh
            unhealthy_services=$(docker compose ps --filter "health!=healthy" --format "{{.Name}}: {{.Health}}")
            if [ -n "$unhealthy_services" ]; then
              echo "Một số dịch vụ Airflow không trở nên khỏe mạnh:"
              echo "$unhealthy_services"
              exit 1
            fi
        

      - name: Verify Airflow Services
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose ps

        

      - name: Trigger DAG via Airflow API
        id: trigger_dag
        run: |
          AIRFLOW_API_URL="http://localhost:8080/api/v1/dags/data_ingestion_aws_project/dagRuns"
          AUTH=$(echo -n "$AIRFLOW_API_USER:$AIRFLOW_API_PASSWORD" | base64)
          RESPONSE=$(curl -s -X POST "$AIRFLOW_API_URL" \
            -H "Content-Type: application/json" \
            -H "Authorization: Basic $AUTH" \
            -d '{"conf": {}}')
          echo "Response: $RESPONSE"
          # Trích xuất dag_run_id từ response
          dag_run_id=$(echo "$RESPONSE" | jq -r '.dag_run_id')
          echo "DAG Run ID: $dag_run_id"
          # Ghi nhận dag_run_id vào biến môi trường GitHub
          echo "DAG_RUN_ID=$dag_run_id" >> $GITHUB_ENV


      - name: Teardown Airflow
        if: always()
        run: |
          cd sources/batch_processing/airflow/dag-airflow
          docker compose down

      - name: Start Kafka
        run: |
          cd sources/stream_processing
          docker compose --env-file .env up -d
          echo "Waiting for Kafka to start..."
          sleep 15

      - name: Run Kafka Producer
        run: |
          cd sources/stream_processing
          python producer.py

      - name: Run Kafka Consumer
        run: |
          cd sources/stream_processing
          python consumer.py

      - name: Teardown Kafka
        if: always()
        run: |
          cd sources/stream_processing
          docker compose down

      - name: Cache dbt packages
        uses: actions/cache@v3
        with:
          path: ~/.dbt
          key: ${{ runner.os }}-dbt-${{ hashFiles('**/packages.yml') }}
          restore-keys: |
            ${{ runner.os }}-dbt-

      - name: Install dbt
        run: |
          pip install dbt-core dbt-snowflake

      - name: dbt run for DWH
        run: |
          cd sources/batch_processing/dbt/marketing_campaign_dwh
          dbt deps
          dbt run
          dbt test

      - name: dbt run for Data Mart
        run: |
          cd sources/batch_processing/dbt/marketing_datamart
          dbt deps
          dbt run
          dbt test

